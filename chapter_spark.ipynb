{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=\"C:\\\\Users\\\\choidaek\\\\Desktop\\\\spark-2.0.2-bin-hadoop2.6\"\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "#sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.3-src.zip'))\n",
    "#sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))\n",
    "#sys.path.remove(\"C:\\Users\\choidaek\\Desktop\\spark-2.0.2-bin-hadoop2.6\\python\\lib\\py4j-0.10.3-src.zip\")\n",
    "#sys.path.remove(\"C:\\Users\\choidaek\\Desktop\\spark-2.0.2-bin-hadoop2.6\\python\\lib\\pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\choidaek\\Desktop\\spark-2.0.2-bin-hadoop2.6\\python\\lib\\py4j-0.10.3-src.zip\n",
      "C:\\Users\\choidaek\\Desktop\\spark-2.0.2-bin-hadoop2.6\\python\\lib\\pyspark.zip\n",
      "\n",
      "C:\\Python34\\Lib\n",
      "C:\\ProgramData\\Anaconda2\\python27.zip\n",
      "C:\\ProgramData\\Anaconda2\\DLLs\n",
      "C:\\ProgramData\\Anaconda2\\lib\n",
      "C:\\ProgramData\\Anaconda2\\lib\\plat-win\n",
      "C:\\ProgramData\\Anaconda2\\lib\\lib-tk\n",
      "C:\\ProgramData\\Anaconda2\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\PyDispatcher-2.0.5-py2.7.egg\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\Sphinx-1.5.1-py2.7.egg\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\characteristic-14.3.0-py2.7.egg\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\cssselect-0.9.1-py2.7.egg\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\pyasn1_modules-0.0.5-py2.7.egg\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\win32\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\win32\\lib\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\Pythonwin\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\service_identity-14.0.0-py2.7.egg\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\setuptools-27.2.0-py2.7.egg\n",
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\IPython\\extensions\n",
      "C:\\Users\\choidaek\\.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name accumulators",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e4ffeaa2d7f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;31m#myConf=pyspark.SparkConf()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m#type(myConf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m#spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).getOrCreate()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\choidaek\\Desktop\\spark-2.0.2-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\choidaek\\Desktop\\spark-2.0.2-bin-hadoop2.6\\python\\lib\\pyspark.zip\\pyspark\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name accumulators"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "type(myConf)\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 6C02-6700\n",
      "\n",
      " c:\\Users\\Code\\s-201111265 디렉터리\n",
      "\n",
      "2017-05-16  오후 04:24    <DIR>          .\n",
      "2017-05-16  오후 04:24    <DIR>          ..\n",
      "2017-04-12  오후 09:54             1,568 .gitignore\n",
      "2017-05-08  오후 12:05    <DIR>          .ipynb_checkpoints\n",
      "2017-03-21  오전 11:13            12,180 chapter1-1.ipynb\n",
      "2017-04-25  오후 03:54            35,707 chapter2-1.ipynb\n",
      "2017-04-25  오후 03:04            16,394 chapter3.ipynb\n",
      "2017-05-02  오후 05:47            72,452 chapter4.ipynb\n",
      "2017-05-16  오후 04:24             3,110 chapter_spark.ipynb\n",
      "2017-04-04  오전 10:28           592,443 chatper2.ipynb\n",
      "2017-04-18  오후 03:44    <DIR>          data\n",
      "2017-03-17  오후 03:54    <DIR>          doc\n",
      "2017-04-04  오후 05:07    <DIR>          extra\n",
      "2017-03-14  오후 04:28    <DIR>          lib\n",
      "2017-05-02  오후 05:49    <DIR>          mymongodb\n",
      "2017-04-17  오전 12:13    <DIR>          project\n",
      "2017-05-02  오후 05:46    <DIR>          src\n",
      "2017-04-17  오후 12:49               117 test.py\n",
      "2017-04-25  오후 03:14               255 test.pyc\n",
      "2017-03-29  오전 10:12           221,415 this.ipynb\n",
      "2017-04-18  오후 05:27             4,941 Untitled.ipynb\n",
      "              11개 파일             960,582 바이트\n",
      "              10개 디렉터리  14,928,433,152 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
